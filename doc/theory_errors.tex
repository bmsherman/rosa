%!TEX root = work-in-progress.tex
\section{}
\label{theory-errors}
We want to find an inductive specification about the output range and output errors.



\paragraph{Notation:} Let
\begin{itemize}
\item $f$ denote the mathematical function $f: \mathbb{R}^n \to \mathbb{R}^n$
\item $\tilde{f}$ be the finite-precision version of $f$
\item $\tilde{x}$ be the finite-precision, actually computed, value corresponding to the
ideal variable $x$
\item $\lambda$ be an upper bound on the initial error, i.e. $\lv x - \tilde{x} \rv$
\item $\sigma$ be the roundoff error on evaluating $f$ with exact inputs, i.e.
  $\lv f(x) - \tilde{f}(x) \rv$
\end{itemize}
where $f$ and $x$ may be vectors, in which case the definitions are component-wise.

\subsection{Division of error}
The overall error on evaluating $f$ in finite-precision arithmetic is
  $\lv f(x) - \tilde{f}(\tilde{x}) \rv$, where $\lv x - \tilde{x} \rv \le \lambda$.

\begin{align}
\lv f(x) - \tilde{f}(\tilde{x}) \rv &=
 \lv f(x) - f(\tilde{x}) + f(\tilde{x}) - \tilde{f}(\tilde{x}) \rv \\
 &\le \lv f(x) - f(\tilde{x}) \rv + \lv f(\tilde{x}) - \tilde{f}(\tilde{x}) \rv
\end{align}

Suppose there is a function $g: \mathbb{R} \to \mathbb{R}$, such that $\lv f(x) - f(y) \rv \le g(\lv x - y \rv)$.
Further note that $\lv f(\tilde{x}) - \tilde{f}(\tilde{x}) \rv$ is the roundoff error on
computing $f$ with exact inputs. Then,

\begin{align}
\label{errorLoopBody}
\lv f(x) - \tilde{f}(\tilde{x}) \rv &\le g(\lv x - \tilde{x} \rv) + \sigma
\end{align}
We have thus separated the overall error into the error from propagating
the initial uncertainty and the roundoff error.

Consider iterating $f$, i.e. we are computing $f^n(x) = f(f(...f(x)))$.

{\bf Claim: }
\begin{equation}
\lv f^n(x) - \tl{f}^n(\tl{x})\rv \le \sigma + \sum^{n - 1}_{i = 1} g^i(\sigma) + g^n(\lv x - \tl{x} \rv)
\end{equation}

We show this by induction. The base case, for $n = 1$ is shown above.
\begin{align}
\lv  f^n(x) - \tl{f}^n(\tl{x})\rv &\le
  \lv f^n(x) - f(\tl{f}^{n-1}(\tl{x})) \rv + \lv f(\tl{f}^{n-1}(\tl{x})) - \tl{f}^n(\tl{x})\rv \\
  &\le g(\lv f^{n-1}(x) - \tl{f}^{n-1}(\tl{x}) \rv) + \sigma \\
  &\le g (\sigma + \sum^{n - 2}_{i = 1} g^i(\sigma) + g^{n-1}(\lv x - \tl{x} \rv)) + \sigma \\
  &\le \sigma + \sum^{n - 1}_{i = 1} g^i(\sigma) + g^n(\lv x - \tl{x} \rv)
\end{align}

\subsection{Lipschitz continuity}
Now consider $g(x) = K \cdot x$, i.e. which yields Lipschitz continuity:
$\lv f(x) - f(y) \rv \le K \lv x - y \rv$.
Note that we need to compute the Lipschitz constant for the mathematical function $f$,
and not for $\tl{f}$.
The expression for the error then becomes
\begin{align}
\lv f^n(x) - \tl{f}^n(\tl{x})\rv \le K^n + \sum^{n-1}_{i=0}K^i \sigma
  = K^n \lambda + \sigma \sum^{n-1}_{i=0} K^i
  = K^n \lambda + \sigma \left(\dfrac{1 - K^n}{1-K} \right)
\end{align}
when $K \ne 1$.
When $K = 1$, $g$ becomes the identity function and so
\begin{align}
\lv f^n(x) - \tl{f}^n(\tl{x})\rv \le \sigma + \sum^{n-1}_{i=1}\sigma + \lambda
= n \cdot \sigma + \lambda
\end{align}

We want to compute the Lipschitz constant for a function $f: \mathbb{R}^n \to \mathbb{R}$,
that is, we are computing one constant for each output of the function.

Let $h: [0, 1] \to \mathbb{R}$ such that $h(\theta) := f(z + \theta(y-z))$.
Then $h(0) = f(z)$ and $h(1) = f(y)$ and
\eqn{
  \frac{d}{d\theta}h(\theta) &= \nabla f(z + \theta(y-z)) \cdot (y-z)
}

By the mean value theorem:
\eqn{
  f(y) - f(z) &= h(1) - h(0) = h'(\zeta) \qquad \text{ where } \eta \in [0, 1] \\
}

\eqn{
   \lv f(y) - f(z) \rv &= \lv h'(\zeta) \rv\\
&\le\lv \nabla f(z + \zeta(y-z)) \cdot (y-z) \rv \\
&\le \sup\limits_{\theta\in [0,1]} \lv \nabla f(z + \theta(y-z))  \cdot (y-z) \rv\\
&\le \sup\limits_{\theta\in [0,1]} \lv \nabla f(z + \theta(y-z))  \rv \rv (y-z) \rv
}

% \eqn{
%   f(y) - f(z) &= h(1) - h(0) = \int^1_0 h'(\theta)d\theta
%   = \int^1_0\nabla f(z + \theta(y-z)) d\theta \cdot (y-z)
% }

% \eqn{
%   \lv f(y) - f(z) \rv &= \lv \int^1_0\nabla f(z + \theta(y-z)) d\theta \cdot (y-z) \rv \\
% &le \int^1_0 \lv \nabla f(z + \theta(y-z))  \cdot (y-z) \rv d\theta \quad
%    \text{ (triangle inequality for integrals)}\\
% &\le \sup\limits_{\theta\in [0,1]} \lv \nabla f(z + \theta(y-z))  \cdot (y-z) \rv \int^1_0 d\theta\\
% \label{eqnG}
% &= \sup\limits_{\theta\in [0,1]} \lv \nabla f(z + \theta(y-z))  \cdot (y-z) \rv \\
% &\le \sup\limits_{\theta\in [0,1]} \lv \nabla f(z + \theta(y-z))  \rv \rv (y-z) \rv
% }

Since $f(y) - f(z) \in \mathbb{R}$, all vectors norms $\lv f(y) - f(z) \rv$
are equivalent to $|f(y) - f(z)|$ and we can choose any norm.

% If possible (and correct!), for precision reasons, we may want to already use Equation \ref{eqnG}  i.e.
% \eqn{
%   g(x) = \sup\limits_{\theta\in [0,1]} \lv \nabla f(z + \theta(y-z))(x)\rv
% }

Thus, in order to compute the Lipschitz constant, we need to bound the gradient of $f$
over the specified input range.
