%!TEX root = work-in-progress.tex
\section{}
\label{theory-errors}
We want to find an inductive specification about the output range and output errors.



\paragraph{Notation:} Let
\begin{itemize}
\item $f$ denote the mathematical function $f: \mathbb{R}^n \to \mathbb{R}^n$ or $f: \mathbb{R}^n \to \mathbb{R}$
\item $\tilde{f}$ be the finite-precision version of $f$
\item $\tilde{x}$ be the finite-precision, actually computed, value corresponding to the
ideal variable $x$
\item $\lambda$ be an upper bound on the initial error, i.e. $\lv x - \tilde{x} \rv$
\item $\sigma$ be the roundoff error on evaluating $f$ with exact inputs, i.e.
  $\lv f(x) - \tilde{f}(x) \rv$
\end{itemize}
where $f$ and $x$ may be vectors, in which case the definitions are component-wise.

\subsection{Division of error}
The overall error on evaluating $f: \mathbb{R}^n \to \mathbb{R}$ in finite-precision arithmetic is
  $\lv f(x) - \tilde{f}(\tilde{x}) \rv$, where $\lv x - \tilde{x} \rv \le \lambda$.

\begin{align}
\lv f(x) - \tilde{f}(\tilde{x}) \rv &=
 \lv f(x) - f(\tilde{x}) + f(\tilde{x}) - \tilde{f}(\tilde{x}) \rv \\
 &\le \lv f(x) - f(\tilde{x}) \rv + \lv f(\tilde{x}) - \tilde{f}(\tilde{x}) \rv
\end{align}

Suppose there is a function $g: \mathbb{R} \to \mathbb{R}$, such that $\lv f(x) - f(y) \rv \le g(\lv x - y \rv)$.
Further note that $\lv f(\tilde{x}) - \tilde{f}(\tilde{x}) \rv$ is the roundoff error on
computing $f$ with exact inputs. Then,

\begin{align}
\label{errorLoopBody}
\lv f(x) - \tilde{f}(\tilde{x}) \rv &\le g(\lv x - \tilde{x} \rv) + \sigma
\end{align}
We have thus separated the overall error into the error from propagating
the initial uncertainty and the roundoff error.

Consider iterating $f$, i.e. we are computing $f^n(x) = f(f(...f(x)))$.

{\bf Claim: }
\begin{equation}
\lv f^n(x) - \tl{f}^n(\tl{x})\rv \le \sigma + \sum^{n - 1}_{i = 1} g^i(\sigma) + g^n(\lv x - \tl{x} \rv)
\end{equation}

We show this by induction. The base case, for $n = 1$ is shown above.
\begin{align}
\lv  f^n(x) - \tl{f}^n(\tl{x})\rv &\le
  \lv f^n(x) - f(\tl{f}^{n-1}(\tl{x})) \rv + \lv f(\tl{f}^{n-1}(\tl{x})) - \tl{f}^n(\tl{x})\rv \\
  &\le g(\lv f^{n-1}(x) - \tl{f}^{n-1}(\tl{x}) \rv) + \sigma \\
  &\le g (\sigma + \sum^{n - 2}_{i = 1} g^i(\sigma) + g^{n-1}(\lv x - \tl{x} \rv)) + \sigma \\
  &\le \sigma + \sum^{n - 1}_{i = 1} g^i(\sigma) + g^n(\lv x - \tl{x} \rv)
\end{align}


\subsection{Multivariate case}
In the above computation, we compute the error for each component separately, i.e. $f: \mathbb{R}^n \to \mathbb{R}$.
Since $g: \mathbb{R} \to \mathbb{R}$ and we are computing $g(\lv x - \tilde{x} \rv$, we are loosing
precision since we do not consider the error contributions from different inputs separately.
For instance, when using the infinity norm, we only consider the largest error.

Now suppose $f, g: \mathbb{R}^n \to \mathbb{R}^n$ and $x, \tilde{x}, \sigma, \lambda \in \mathbb{R}^n$.

We want to compute the error on every component of $f$, i.e. $\lv f_i(x) - \tilde{f}_i(\tilde{x}) \rv$,
where $\lv x_i - \tilde{x}_i \rv \le \lambda_i$, for $i \in 1 \dots n$.


\begin{align}
\lv f_i(x) - \tilde{f}_i(\tilde{x}) \rv &=
 \lv f_i(x) - f_i(\tilde{x}) + f_i(\tilde{x}) - \tilde{f}_i(\tilde{x}) \rv \\
 &\le \lv f_i(x) - f_i(\tilde{x}) \rv + \lv f_i(\tilde{x}) - \tilde{f}_i(\tilde{x}) \rv
\end{align}

$g: \mathbb{R}^n \to \mathbb{R}^n$, such that
$\lv f_i(x) - f_i(y) \rv \le g_i(\lv x_1 - y_1\rv, ..., \lv x_n - y_n\rv)$
and $\sigma_i = \lv f_i(\tilde{x}) - \tilde{f}_i(\tilde{x}) \rv$, the roundoff error on
computing $f$ with exact inputs. Then,

\eqn{
\label{errorLoopBodyMulti}
\norm{ f_i(x) - \tilde{f}_i(\tilde{x})} &\le g_i(\norm{ x_1 - y_1}, ..., \norm{ x_n - y_n }) + \sigma_i\\
&= g_i(\vec{\lambda}) + \sigma_i
}


Consider iterating $f$, i.e. we are computing $f^n(x) = f(f(...f(x)))$.

{\bf Claim: }
\eqn{
\norm{ f^m(x)_i - \tl{f}^m(\tl{x})_i } \le \left( \sigma + \sum^{m - 1}_{j = 1} g^j(\sigma) + g^m(\norm{x - \tl{x}}) \right)_i
}

We show this by induction. The base case, for $n = 1$ is shown above.
\eqn{
  \colvec{\norm{f^m(x)_1 - \tl{f}^m(\tl{x})_1}}{\vdots}{\norm{f^m(x)_n - \tl{f}^m(\tl{x})_n}} &\le
  \colvec{
  \norm{f^m(x)_1 - f(\tl{f}^{m-1}(\tl{x}))_1} + \norm{f(\tl{f}^{m-1}(\tl{x}))_1 -\tl{f}^m(\tl{x}_1) }
  }{\vdots}{
  \norm{f^m(x)_n - f(\tl{f}^{m-1}(\tl{x}))_n} + \norm{f(\tl{f}^{m-1}(\tl{x}))_n -\tl{f}^m(\tl{x}_n) }}\\
  &\le g\colvec{
  \norm{f^m(x)_1 - f(\tl{f}^{m-1}(\tl{x}))_1}}{\vdots}{\norm{f^m(x)_n - f(\tl{f}^{m-1}(\tl{x}))_n}} +
  \colvec{\sigma_1}{\vdots}{\sigma_n}\\
  &\le g \left(\vec{\sigma} + \sum^{m-2}_{j = 1} g^j(\vec{\sigma}) + g^{m-1}(\vec{\lambda}) \right)+ \vec{\sigma}\\
  &=\sum^{m-1}_{j = 1} g^j(\vec{\sigma}) + g^m(\vec{\lambda}) + \vec{\sigma}
}



\subsection{Lipschitz continuity}
Now consider $g(x) = K \cdot x$, i.e. which yields Lipschitz continuity:
$\lv f(x) - f(y) \rv \le K \lv x - y \rv$.
Note that we need to compute the Lipschitz constant for the mathematical function $f$,
and not for $\tl{f}$.
The expression for the error then becomes
\begin{align}
\lv f^n(x) - \tl{f}^n(\tl{x})\rv \le K^n \lambda + \sum^{n-1}_{i=0}K^i \sigma
  = K^n \lambda + \sigma \sum^{n-1}_{i=0} K^i
  = K^n \lambda + \sigma \left(\dfrac{1 - K^n}{1-K} \right)
\end{align}
when $K \ne 1$.
When $K = 1$, $g$ becomes the identity function and so
\begin{align}
\lv f^n(x) - \tl{f}^n(\tl{x})\rv \le \sigma + \sum^{n-1}_{i=1}\sigma + \lambda
= n \cdot \sigma + \lambda
\end{align}

We want to compute the Lipschitz constant for a function $f: \mathbb{R}^n \to \mathbb{R}$,
that is, we are computing one constant for each output of the function.

Let $h: [0, 1] \to \mathbb{R}$ such that $h(\theta) := f(z + \theta(y-z))$.
Then $h(0) = f(z)$ and $h(1) = f(y)$ and
\eqn{
  \frac{d}{d\theta}h(\theta) &= \nabla f(z + \theta(y-z)) \cdot (y-z)
}

By the mean value theorem:
\eqn{
  f(y) - f(z) &= h(1) - h(0) = h'(\zeta) \qquad \text{ where } \eta \in [0, 1] \\
}

\eqn{
   \lv f(y) - f(z) \rv &= \lv h'(\zeta) \rv\\
&\le\lv \nabla f(z + \zeta(y-z)) \cdot (y-z) \rv \\
&\le \sup\limits_{\theta\in [0,1]} \lv \nabla f(z + \theta(y-z))  \cdot (y-z) \rv\\
&\le \sup\limits_{\theta\in [0,1]} \lv \nabla f(z + \theta(y-z))  \rv \rv (y-z) \rv
}

% \eqn{
%   f(y) - f(z) &= h(1) - h(0) = \int^1_0 h'(\theta)d\theta
%   = \int^1_0\nabla f(z + \theta(y-z)) d\theta \cdot (y-z)
% }

% \eqn{
%   \lv f(y) - f(z) \rv &= \lv \int^1_0\nabla f(z + \theta(y-z)) d\theta \cdot (y-z) \rv \\
% &le \int^1_0 \lv \nabla f(z + \theta(y-z))  \cdot (y-z) \rv d\theta \quad
%    \text{ (triangle inequality for integrals)}\\
% &\le \sup\limits_{\theta\in [0,1]} \lv \nabla f(z + \theta(y-z))  \cdot (y-z) \rv \int^1_0 d\theta\\
% \label{eqnG}
% &= \sup\limits_{\theta\in [0,1]} \lv \nabla f(z + \theta(y-z))  \cdot (y-z) \rv \\
% &\le \sup\limits_{\theta\in [0,1]} \lv \nabla f(z + \theta(y-z))  \rv \rv (y-z) \rv
% }

Since $f(y) - f(z) \in \mathbb{R}$, all vectors norms $\lv f(y) - f(z) \rv$
are equivalent to $|f(y) - f(z)|$ and we can choose any norm.

% If possible (and correct!), for precision reasons, we may want to already use Equation \ref{eqnG}  i.e.
% \eqn{
%   g(x) = \sup\limits_{\theta\in [0,1]} \lv \nabla f(z + \theta(y-z))(x)\rv
% }

Thus, in order to compute the Lipschitz constant, we need to bound the gradient of $f$
over the specified input range.



\subsubsection{Multivariate case}
Let $h: [0, 1] \to \mathbb{R}^n$ such that $h(\theta) := f(z + \theta(y-z))$.
Then $h(0) = f(z)$ and $h(1) = f(y)$ and
\eqn{
  \frac{d}{d\theta}h(\theta) &= \nabla f(z + \theta(y-z)) \cdot (y-z)
}

By the mean value theorem:
\eqn{
  f(y) - f(z) &= h(1) - h(0) = h'(\zeta) \qquad \text{ where } \eta \in [0, 1] \\
}
\eqn{
   \norm{ f_i(y) - f_i(z) } &= \norm{ h_i'(\zeta) }\\
&\le \norm{ \left( \nabla f(z + \zeta(y-z)) \cdot (y-z) \right)_i } \\
&= \norm{ \left(
\begin{pmatrix}
\partialDer{f_1}{w_1} & \dots & \partialDer{f_1}{w_n}\\
\dots \\
\partialDer{f_n}{w_1} & \dots & \partialDer{f_n}{w_n}
\end{pmatrix}\cdot (y-z) \right)_i }\\
&= \norm{
\partialDer{f_i}{w_1} \cdot (y_1 - z_1) + \dots + \partialDer{f_i}{w_1} \cdot (y_n - z_n)}\\
%&\le \sup\limits_{\theta\in [0,1]} \lv \nabla f(z + \theta(y-z))  \cdot (y-z) \rv\\
%&\le \sup\limits_{\theta\in [0,1]} \lv \nabla f(z + \theta(y-z))  \rv \rv (y-z) \rv
}

Hence, we get
\begin{align}
\lv f^n(x) - \tl{f}^n(\tl{x})\rv \le K^n \lambda + \sum^{n-1}_{i=0}K^i \sigma
  = K^n \lambda + \sigma \sum^{n-1}_{i=0} K^i
  = K^n \lambda + \sigma (I-K)^{-1}(I - K^n)
\end{align}
where $K$ is now a matrix with $K_{ij} = \partialDer{f_i}{w_j}$.
